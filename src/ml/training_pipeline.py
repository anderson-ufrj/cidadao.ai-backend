"""
Pipeline de Treinamento para CidadÃ£o.AI

Sistema completo de fine-tuning especializado para dados de transparÃªncia pÃºblica brasileira.
Inspirado nas tÃ©cnicas do Kimi K2, mas otimizado para anÃ¡lise governamental.
"""

import os
from src.core import json_utils
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from transformers import AutoTokenizer, get_linear_schedule_with_warmup
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from dataclasses import dataclass, asdict
from tqdm import tqdm
import wandb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from .cidadao_model import CidadaoAIForTransparency, CidadaoModelConfig, create_cidadao_model

logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """ConfiguraÃ§Ã£o de treinamento"""
    
    # HiperparÃ¢metros principais
    learning_rate: float = 2e-5
    batch_size: int = 8
    num_epochs: int = 10
    warmup_steps: int = 1000
    max_grad_norm: float = 1.0
    weight_decay: float = 0.01
    
    # ConfiguraÃ§Ãµes de dados
    max_sequence_length: int = 512
    train_split: float = 0.8
    val_split: float = 0.1
    test_split: float = 0.1
    
    # ConfiguraÃ§Ãµes do modelo
    model_size: str = "medium"
    specialized_tasks: List[str] = None
    use_mixed_precision: bool = True
    gradient_accumulation_steps: int = 4
    
    # ConfiguraÃ§Ãµes de checkpoint
    save_strategy: str = "epoch"  # "steps" ou "epoch"
    save_steps: int = 500
    eval_steps: int = 100
    logging_steps: int = 50
    output_dir: str = "./models/cidadao-gpt"
    
    # ConfiguraÃ§Ãµes de avaliaÃ§Ã£o
    eval_strategy: str = "steps"
    metric_for_best_model: str = "eval_f1"
    greater_is_better: bool = True
    early_stopping_patience: int = 3
    
    # ConfiguraÃ§Ãµes de experimentaÃ§Ã£o
    experiment_name: str = "cidadao-gpt-v1"
    use_wandb: bool = True
    wandb_project: str = "cidadao-ai"
    
    def __post_init__(self):
        if self.specialized_tasks is None:
            self.specialized_tasks = ["all"]


class TransparencyDataset(Dataset):
    """Dataset especializado para dados de transparÃªncia pÃºblica"""
    
    def __init__(
        self,
        data_path: str,
        tokenizer: AutoTokenizer,
        max_length: int = 512,
        task_type: str = "multi_task"
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.task_type = task_type
        
        # Carregar dados
        self.data = self._load_data(data_path)
        
        # Preparar vocabulÃ¡rio especializado
        self._prepare_specialized_vocab()
        
    def _load_data(self, data_path: str) -> List[Dict]:
        """Carregar dados de transparÃªncia"""
        
        data_file = Path(data_path)
        
        if data_file.suffix == '.json':
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json_utils.load(f)
        elif data_file.suffix == '.jsonl':
            data = []
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    data.append(json_utils.loads(line))
        else:
            # Assumir dados do Portal da TransparÃªncia em formato estruturado
            data = self._load_transparency_data(data_path)
            
        logger.info(f"Carregados {len(data)} exemplos de {data_path}")
        return data
    
    def _load_transparency_data(self, data_path: str) -> List[Dict]:
        """Carregar dados reais do Portal da TransparÃªncia"""
        
        # Simular estrutura de dados reais
        # Em produÃ§Ã£o, isso seria conectado ao pipeline de dados real
        sample_data = []
        
        # Exemplos de contratos com diferentes tipos de problemas
        contract_examples = [
            {
                "text": "Contrato para aquisiÃ§Ã£o de equipamentos mÃ©dicos no valor de R$ 2.500.000,00 firmado entre MinistÃ©rio da SaÃºde e Empresa XYZ LTDA. Processo licitatÃ³rio 12345/2024, modalidade pregÃ£o eletrÃ´nico.",
                "anomaly_label": 0,  # Normal
                "financial_risk": 2,  # MÃ©dio
                "legal_compliance": 1,  # Conforme
                "contract_value": 2500000.0,
                "entity_types": [1, 2, 3],  # MinistÃ©rio, Empresa, Equipamento
                "corruption_indicators": []
            },
            {
                "text": "Contrato emergencial sem licitaÃ§Ã£o para fornecimento de insumos hospitalares. Valor: R$ 15.000.000,00. Empresa beneficiÃ¡ria: Alpha Beta Comercial S.A., CNPJ com irregularidades na Receita Federal.",
                "anomaly_label": 2,  # AnÃ´malo
                "financial_risk": 4,  # Alto
                "legal_compliance": 0,  # NÃ£o conforme
                "contract_value": 15000000.0,
                "entity_types": [1, 2, 4],  # MinistÃ©rio, Empresa, Insumos
                "corruption_indicators": [1, 3, 5]  # Emergencial, Sem licitaÃ§Ã£o, CNPJ irregular
            }
        ]
        
        # Amplificar dados com variaÃ§Ãµes
        for base_example in contract_examples:
            for i in range(50):  # 50 variaÃ§Ãµes de cada exemplo
                example = base_example.copy()
                example["id"] = f"{len(sample_data)}"
                
                # Adicionar ruÃ­do realÃ­stico
                if np.random.random() > 0.5:
                    example["text"] = self._add_realistic_variations(example["text"])
                
                sample_data.append(example)
        
        return sample_data
    
    def _add_realistic_variations(self, text: str) -> str:
        """Adicionar variaÃ§Ãµes realÃ­sticas ao texto"""
        
        variations = [
            text.replace("MinistÃ©rio da SaÃºde", "MS"),
            text.replace("equipamentos mÃ©dicos", "equipamentos hospitalares"),
            text.replace("pregÃ£o eletrÃ´nico", "concorrÃªncia pÃºblica"),
            text + " Processo administrativo arquivado em sistema SIASG.",
            text + " Valor atualizado conforme INPC/IBGE."
        ]
        
        return np.random.choice(variations)
    
    def _prepare_specialized_vocab(self):
        """Preparar vocabulÃ¡rio especializado para transparÃªncia"""
        
        # Termos tÃ©cnicos de transparÃªncia pÃºblica
        self.transparency_terms = {
            # Entidades
            "ministerio", "secretaria", "orgao", "entidade", "empresa", "fornecedor",
            
            # Tipos de contrato
            "licitacao", "pregao", "concorrencia", "tomada_precos", "convite", "dispensa",
            
            # Indicadores financeiros
            "valor", "preco", "orcamento", "pagamento", "repasse", "empenho",
            
            # Termos jurÃ­dicos
            "conformidade", "irregularidade", "infraÃ§Ã£o", "penalidade", "multa",
            
            # Indicadores de corrupÃ§Ã£o
            "superfaturamento", "direcionamento", "cartel", "fraude", "peculato"
        }
        
        # Adicionar tokens especiais se necessÃ¡rio
        special_tokens = ["[CONTRACT]", "[ENTITY]", "[VALUE]", "[ANOMALY]", "[LEGAL]"]
        self.tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = self.data[idx]
        
        # Tokenizar texto
        encoding = self.tokenizer(
            item["text"],
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        # Preparar labels e features especializadas
        result = {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
        }
        
        # Adicionar labels especÃ­ficos por tarefa
        if "anomaly_label" in item:
            result["anomaly_labels"] = torch.tensor(item["anomaly_label"], dtype=torch.long)
            
        if "financial_risk" in item:
            result["financial_risk_labels"] = torch.tensor(item["financial_risk"], dtype=torch.long)
            
        if "legal_compliance" in item:
            result["legal_compliance_labels"] = torch.tensor(item["legal_compliance"], dtype=torch.long)
        
        # Adicionar features especializadas
        if "entity_types" in item:
            entity_types = torch.zeros(self.max_length, dtype=torch.long)
            for i, entity_type in enumerate(item["entity_types"][:self.max_length]):
                entity_types[i] = entity_type
            result["entity_types"] = entity_types
            
        if "corruption_indicators" in item:
            corruption_indicators = torch.zeros(self.max_length, dtype=torch.long)
            for i, indicator in enumerate(item["corruption_indicators"][:self.max_length]):
                corruption_indicators[i] = indicator
            result["corruption_indicators"] = corruption_indicators
        
        return result


class CidadaoTrainer:
    """Trainer especializado para CidadÃ£o.AI"""
    
    def __init__(
        self,
        model: CidadaoAIForTransparency,
        tokenizer: AutoTokenizer,
        config: TrainingConfig
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        
        # Configurar device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # Configurar otimizador
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # Configurar mixed precision se disponÃ­vel
        self.scaler = torch.cuda.amp.GradScaler() if config.use_mixed_precision else None
        
        # MÃ©tricas de treinamento
        self.training_history = {
            "train_loss": [],
            "eval_loss": [],
            "eval_metrics": []
        }
        
        # Early stopping
        self.best_metric = float('-inf') if config.greater_is_better else float('inf')
        self.patience_counter = 0
        
        # Configurar logging
        if config.use_wandb:
            wandb.init(
                project=config.wandb_project,
                name=config.experiment_name,
                config=asdict(config)
            )
    
    def train(
        self,
        train_dataset: TransparencyDataset,
        eval_dataset: Optional[TransparencyDataset] = None,
        test_dataset: Optional[TransparencyDataset] = None
    ):
        """Executar treinamento completo"""
        
        logger.info("ğŸš€ Iniciando treinamento do CidadÃ£o.AI")
        
        # Preparar data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=4
        )
        
        eval_loader = None
        if eval_dataset:
            eval_loader = DataLoader(
                eval_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                num_workers=4
            )
        
        # Configurar scheduler
        total_steps = len(train_loader) * self.config.num_epochs
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=self.config.warmup_steps,
            num_training_steps=total_steps
        )
        
        # Loop de treinamento
        global_step = 0
        
        for epoch in range(self.config.num_epochs):
            logger.info(f"ğŸ“š Ã‰poca {epoch + 1}/{self.config.num_epochs}")
            
            # Treinamento
            train_loss = self._train_epoch(train_loader, epoch, global_step)
            self.training_history["train_loss"].append(train_loss)
            
            # AvaliaÃ§Ã£o
            if eval_loader and (epoch + 1) % 1 == 0:  # Avaliar a cada Ã©poca
                eval_metrics = self._evaluate(eval_loader, epoch)
                self.training_history["eval_metrics"].append(eval_metrics)
                
                # Early stopping check
                current_metric = eval_metrics[self.config.metric_for_best_model]
                if self._is_better_metric(current_metric):
                    self.best_metric = current_metric
                    self.patience_counter = 0
                    self._save_checkpoint(epoch, is_best=True)
                    logger.info(f"ğŸ¯ Novo melhor modelo! {self.config.metric_for_best_model}: {current_metric:.4f}")
                else:
                    self.patience_counter += 1
                    
                if self.patience_counter >= self.config.early_stopping_patience:
                    logger.info(f"â° Early stopping acionado apÃ³s {self.patience_counter} Ã©pocas sem melhoria")
                    break
            
            # Salvar checkpoint regular
            if (epoch + 1) % 2 == 0:  # Salvar a cada 2 Ã©pocas
                self._save_checkpoint(epoch, is_best=False)
            
            global_step += len(train_loader)
        
        # AvaliaÃ§Ã£o final
        if test_dataset:
            test_loader = DataLoader(
                test_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                num_workers=4
            )
            
            logger.info("ğŸ§ª Executando avaliaÃ§Ã£o final no conjunto de teste")
            final_metrics = self._evaluate(test_loader, epoch=-1, is_test=True)
            
            logger.info("ğŸ“Š MÃ©tricas finais:")
            for metric, value in final_metrics.items():
                logger.info(f"  {metric}: {value:.4f}")
        
        # Finalizar treinamento
        self._finalize_training()
    
    def _train_epoch(self, train_loader: DataLoader, epoch: int, global_step: int) -> float:
        """Treinar uma Ã©poca"""
        
        self.model.train()
        total_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f"Treinamento Ã‰poca {epoch + 1}")
        
        for step, batch in enumerate(progress_bar):
            # Mover dados para device
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # Forward pass com mixed precision
            if self.scaler:
                with torch.cuda.amp.autocast():
                    loss = self._compute_multi_task_loss(batch)
            else:
                loss = self._compute_multi_task_loss(batch)
            
            # Backward pass
            if self.scaler:
                self.scaler.scale(loss).backward()
                
                if (step + 1) % self.config.gradient_accumulation_steps == 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
            else:
                loss.backward()
                
                if (step + 1) % self.config.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
            
            total_loss += loss.item()
            
            # Logging
            if step % self.config.logging_steps == 0:
                avg_loss = total_loss / (step + 1)
                progress_bar.set_postfix({"loss": f"{avg_loss:.4f}"})
                
                if self.config.use_wandb:
                    wandb.log({
                        "train/loss": avg_loss,
                        "train/learning_rate": self.scheduler.get_last_lr()[0],
                        "train/epoch": epoch,
                        "train/step": global_step + step
                    })
        
        return total_loss / len(train_loader)
    
    def _compute_multi_task_loss(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Computar loss multi-tarefa"""
        
        total_loss = 0.0
        loss_weights = {
            "anomaly": 1.0,
            "financial": 0.8,
            "legal": 0.6
        }
        
        # Loss de detecÃ§Ã£o de anomalias
        if "anomaly_labels" in batch:
            anomaly_outputs = self.model.detect_anomalies(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                entity_types=batch.get("entity_types"),
                corruption_indicators=batch.get("corruption_indicators")
            )
            
            # Extrair logits dos resultados
            anomaly_logits = []
            for pred in anomaly_outputs["predictions"]:
                probs = [
                    pred["probabilities"]["normal"],
                    pred["probabilities"]["suspicious"], 
                    pred["probabilities"]["anomalous"]
                ]
                anomaly_logits.append(probs)
            
            anomaly_logits = torch.tensor(anomaly_logits, device=self.device)
            anomaly_loss = nn.CrossEntropyLoss()(anomaly_logits, batch["anomaly_labels"])
            total_loss += loss_weights["anomaly"] * anomaly_loss
        
        # Loss de anÃ¡lise financeira
        if "financial_risk_labels" in batch:
            financial_outputs = self.model.analyze_financial_risk(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            
            # Extrair logits dos resultados
            risk_logits = []
            for pred in financial_outputs["predictions"]:
                probs = list(pred["risk_probabilities"].values())
                risk_logits.append(probs)
            
            risk_logits = torch.tensor(risk_logits, device=self.device)
            financial_loss = nn.CrossEntropyLoss()(risk_logits, batch["financial_risk_labels"])
            total_loss += loss_weights["financial"] * financial_loss
        
        # Loss de conformidade legal
        if "legal_compliance_labels" in batch:
            legal_outputs = self.model.check_legal_compliance(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            
            # Extrair logits dos resultados
            compliance_logits = []
            for pred in legal_outputs["predictions"]:
                probs = [
                    pred["legal_analysis"]["non_compliant_prob"],
                    pred["legal_analysis"]["compliant_prob"]
                ]
                compliance_logits.append(probs)
            
            compliance_logits = torch.tensor(compliance_logits, device=self.device)
            legal_loss = nn.CrossEntropyLoss()(compliance_logits, batch["legal_compliance_labels"])
            total_loss += loss_weights["legal"] * legal_loss
        
        return total_loss
    
    def _evaluate(self, eval_loader: DataLoader, epoch: int, is_test: bool = False) -> Dict[str, float]:
        """Avaliar modelo"""
        
        self.model.eval()
        total_loss = 0.0
        
        # Coletar prediÃ§Ãµes e labels
        all_predictions = {
            "anomaly": {"preds": [], "labels": []},
            "financial": {"preds": [], "labels": []},
            "legal": {"preds": [], "labels": []}
        }
        
        with torch.no_grad():
            for batch in tqdm(eval_loader, desc="AvaliaÃ§Ã£o"):
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # Computar loss
                loss = self._compute_multi_task_loss(batch)
                total_loss += loss.item()
                
                # Coletar prediÃ§Ãµes
                self._collect_predictions(batch, all_predictions)
        
        avg_loss = total_loss / len(eval_loader)
        
        # Computar mÃ©tricas
        metrics = {"eval_loss": avg_loss}
        
        for task, preds_labels in all_predictions.items():
            if preds_labels["preds"]:
                task_metrics = self._compute_task_metrics(
                    preds_labels["preds"], 
                    preds_labels["labels"],
                    task_name=task
                )
                metrics.update(task_metrics)
        
        # Logging
        prefix = "test" if is_test else "eval"
        log_metrics = {f"{prefix}/{k}": v for k, v in metrics.items()}
        
        if self.config.use_wandb:
            wandb.log(log_metrics)
        
        return metrics
    
    def _collect_predictions(self, batch: Dict[str, torch.Tensor], all_predictions: Dict):
        """Coletar prediÃ§Ãµes para avaliaÃ§Ã£o"""
        
        # Anomaly detection
        if "anomaly_labels" in batch:
            anomaly_outputs = self.model.detect_anomalies(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            
            for i, pred in enumerate(anomaly_outputs["predictions"]):
                anomaly_type_map = {"Normal": 0, "Suspeito": 1, "AnÃ´malo": 2}
                pred_label = anomaly_type_map[pred["anomaly_type"]]
                all_predictions["anomaly"]["preds"].append(pred_label)
                all_predictions["anomaly"]["labels"].append(batch["anomaly_labels"][i].item())
        
        # Financial analysis
        if "financial_risk_labels" in batch:
            financial_outputs = self.model.analyze_financial_risk(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            
            for i, pred in enumerate(financial_outputs["predictions"]):
                risk_level_map = {"Muito Baixo": 0, "Baixo": 1, "MÃ©dio": 2, "Alto": 3, "Muito Alto": 4}
                pred_label = risk_level_map[pred["risk_level"]]
                all_predictions["financial"]["preds"].append(pred_label)
                all_predictions["financial"]["labels"].append(batch["financial_risk_labels"][i].item())
        
        # Legal compliance
        if "legal_compliance_labels" in batch:
            legal_outputs = self.model.check_legal_compliance(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            
            for i, pred in enumerate(legal_outputs["predictions"]):
                pred_label = 1 if pred["is_compliant"] else 0
                all_predictions["legal"]["preds"].append(pred_label)
                all_predictions["legal"]["labels"].append(batch["legal_compliance_labels"][i].item())
    
    def _compute_task_metrics(self, predictions: List, labels: List, task_name: str) -> Dict[str, float]:
        """Computar mÃ©tricas para uma tarefa especÃ­fica"""
        
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='weighted'
        )
        
        metrics = {
            f"eval_{task_name}_accuracy": accuracy,
            f"eval_{task_name}_precision": precision,
            f"eval_{task_name}_recall": recall,
            f"eval_{task_name}_f1": f1
        }
        
        # MÃ©trica composta para early stopping
        if task_name == "anomaly":  # Usar anomaly como principal
            metrics["eval_f1"] = f1
        
        return metrics
    
    def _is_better_metric(self, current_metric: float) -> bool:
        """Verificar se mÃ©trica atual Ã© melhor"""
        if self.config.greater_is_better:
            return current_metric > self.best_metric
        else:
            return current_metric < self.best_metric
    
    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """Salvar checkpoint do modelo"""
        
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if is_best:
            save_path = output_dir / "best_model"
        else:
            save_path = output_dir / f"checkpoint-epoch-{epoch}"
        
        # Salvar modelo
        self.model.save_model(str(save_path))
        
        # Salvar estado do treinamento
        training_state = {
            "epoch": epoch,
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "best_metric": self.best_metric,
            "training_history": self.training_history
        }
        
        torch.save(training_state, save_path / "training_state.pt")
        
        logger.info(f"âœ… Checkpoint salvo em {save_path}")
    
    def _finalize_training(self):
        """Finalizar treinamento"""
        
        # Salvar histÃ³rico de treinamento
        output_dir = Path(self.config.output_dir)
        
        with open(output_dir / "training_history.json", "w") as f:
            json_utils.dump(self.training_history, f, indent=2)
        
        # Plotar curvas de treinamento
        self._plot_training_curves()
        
        if self.config.use_wandb:
            wandb.finish()
        
        logger.info("ğŸ‰ Treinamento finalizado com sucesso!")
    
    def _plot_training_curves(self):
        """Plotar curvas de treinamento"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss de treinamento
        epochs = range(1, len(self.training_history["train_loss"]) + 1)
        axes[0, 0].plot(epochs, self.training_history["train_loss"])
        axes[0, 0].set_title("Loss de Treinamento")
        axes[0, 0].set_xlabel("Ã‰poca")
        axes[0, 0].set_ylabel("Loss")
        
        # MÃ©tricas de avaliaÃ§Ã£o
        if self.training_history["eval_metrics"]:
            eval_epochs = range(1, len(self.training_history["eval_metrics"]) + 1)
            
            # F1 Score
            f1_scores = [m.get("eval_f1", 0) for m in self.training_history["eval_metrics"]]
            axes[0, 1].plot(eval_epochs, f1_scores, 'g-')
            axes[0, 1].set_title("F1 Score")
            axes[0, 1].set_xlabel("Ã‰poca")
            axes[0, 1].set_ylabel("F1")
            
            # Accuracy
            accuracy_scores = [m.get("eval_anomaly_accuracy", 0) for m in self.training_history["eval_metrics"]]
            axes[1, 0].plot(eval_epochs, accuracy_scores, 'b-')
            axes[1, 0].set_title("Accuracy")
            axes[1, 0].set_xlabel("Ã‰poca")
            axes[1, 0].set_ylabel("Accuracy")
            
            # Loss de avaliaÃ§Ã£o
            eval_losses = [m.get("eval_loss", 0) for m in self.training_history["eval_metrics"]]
            axes[1, 1].plot(eval_epochs, eval_losses, 'r-')
            axes[1, 1].set_title("Loss de AvaliaÃ§Ã£o")
            axes[1, 1].set_xlabel("Ã‰poca")
            axes[1, 1].set_ylabel("Loss")
        
        plt.tight_layout()
        
        # Salvar plot
        output_dir = Path(self.config.output_dir)
        plt.savefig(output_dir / "training_curves.png", dpi=300, bbox_inches='tight')
        plt.close()


def create_training_pipeline(
    data_path: str,
    config: Optional[TrainingConfig] = None
) -> Tuple[CidadaoAIForTransparency, CidadaoTrainer]:
    """
    Criar pipeline de treinamento completo
    
    Args:
        data_path: Caminho para dados de treinamento
        config: ConfiguraÃ§Ã£o de treinamento
    
    Returns:
        Tuple com modelo e trainer
    """
    
    if config is None:
        config = TrainingConfig()
    
    logger.info("ğŸ—ï¸ Criando pipeline de treinamento CidadÃ£o.AI")
    
    # Criar modelo
    model = create_cidadao_model(
        specialized_tasks=config.specialized_tasks,
        model_size=config.model_size
    )
    
    # Criar tokenizer
    tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
    tokenizer.pad_token = tokenizer.eos_token
    
    # Redimensionar embeddings se necessÃ¡rio
    model.model.model.resize_token_embeddings(len(tokenizer))
    
    # Criar trainer
    trainer = CidadaoTrainer(model, tokenizer, config)
    
    logger.info(f"âœ… Pipeline criado - Modelo: {config.model_size}, Tarefas: {config.specialized_tasks}")
    
    return model, trainer


def prepare_transparency_data(data_path: str, output_dir: str = "./data/processed"):
    """
    Preparar dados de transparÃªncia para treinamento
    
    Esta funÃ§Ã£o seria expandida para processar dados reais do Portal da TransparÃªncia
    """
    
    logger.info("ğŸ“Š Preparando dados de transparÃªncia")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Aqui vocÃª implementaria:
    # 1. ConexÃ£o com Portal da TransparÃªncia API
    # 2. ExtraÃ§Ã£o e limpeza de dados
    # 3. AnotaÃ§Ã£o de anomalias (semi-supervisionado)
    # 4. Balanceamento de classes
    # 5. DivisÃ£o train/val/test
    
    # Por enquanto, criar dados sintÃ©ticos
    logger.info("âš ï¸ Usando dados sintÃ©ticos para demonstraÃ§Ã£o")
    
    # ImplementaÃ§Ã£o completa seria conectada aos dados reais
    sample_data = {
        "train": output_dir / "train.json",
        "val": output_dir / "val.json", 
        "test": output_dir / "test.json"
    }
    
    return sample_data


if __name__ == "__main__":
    # Exemplo de uso
    
    # Configurar logging
    logging.basicConfig(level=logging.INFO)
    
    # ConfiguraÃ§Ã£o de treinamento
    config = TrainingConfig(
        experiment_name="cidadao-gpt-transparency-v1",
        num_epochs=5,
        batch_size=4,  # Reduzido para teste
        learning_rate=2e-5,
        use_wandb=False,  # Desabilitar para teste
        output_dir="./models/cidadao-gpt-test"
    )
    
    # Criar pipeline
    model, trainer = create_training_pipeline(
        data_path="./data/transparency_data.json",
        config=config
    )
    
    print("ğŸ¤– CidadÃ£o.AI Training Pipeline criado com sucesso!")
    print(f"ğŸ“Š Modelo: {config.model_size}")
    print(f"ğŸ¯ Tarefas especializadas: {config.specialized_tasks}")
    print(f"ğŸ’¾ DiretÃ³rio de saÃ­da: {config.output_dir}")