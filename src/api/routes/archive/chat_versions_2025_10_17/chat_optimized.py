"""
Optimized chat endpoint with Sabiazinho model and Drummond persona
More economical and culturally enriched responses
"""

import os
import uuid
from datetime import UTC, datetime
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from src.core import get_logger
from src.services.maritaca_client import MaritacaClient, MaritacaModel

logger = get_logger(__name__)
router = APIRouter(tags=["chat-optimized"])

# Initialize Maritaca client with Sabiazinho
maritaca_client = None
MARITACA_API_KEY = os.environ.get("MARITACA_API_KEY")

if MARITACA_API_KEY:
    try:
        # Use Sabiazinho for more economical operation
        maritaca_client = MaritacaClient(
            api_key=MARITACA_API_KEY,
            model=MaritacaModel.SABIAZINHO_3,  # More economical model
        )
        logger.info("Maritaca AI client initialized with Sabiazinho model")
    except Exception as e:
        logger.error(f"Failed to initialize Maritaca AI: {e}")
else:
    logger.warning("MARITACA_API_KEY not found - chat will use fallback responses")


class OptimizedChatRequest(BaseModel):
    """Optimized chat request"""

    message: str = Field(..., min_length=1, max_length=1000)
    session_id: str | None = None
    use_drummond: bool = Field(default=True, description="Use Drummond persona")


class OptimizedChatResponse(BaseModel):
    """Optimized chat response"""

    message: str
    session_id: str
    agent_name: str
    agent_id: str
    timestamp: str
    model_used: str
    confidence: float
    metadata: dict[str, Any]


# Drummond-inspired system prompt
DRUMMOND_PROMPT = """Voc√™ √© Carlos Drummond de Andrade, o poeta modernista brasileiro, agora servindo como assistente de transpar√™ncia p√∫blica.

Com sua sensibilidade po√©tica e olhar cr√≠tico sobre a sociedade brasileira, voc√™ ajuda cidad√£os a compreender os meandros dos gastos p√∫blicos.

Caracter√≠sticas do seu estilo:
- Mantenha o tom acolhedor e humanizado de Drummond
- Use met√°foras po√©ticas ocasionais para ilustrar conceitos complexos
- Seja claro e objetivo, mas com a profundidade caracter√≠stica do poeta
- Demonstre empatia com as preocupa√ß√µes do cidad√£o
- Explique dados t√©cnicos com a clareza de quem escreve para o povo

Ao responder sobre transpar√™ncia:
- Ajude a investigar contratos, licita√ß√µes e gastos p√∫blicos
- Explique conceitos do Portal da Transpar√™ncia
- Identifique poss√≠veis anomalias ou padr√µes suspeitos
- Sugira caminhos para o cidad√£o exercer seu direito √† informa√ß√£o

Lembre-se:
- Evite julgamentos pol√≠ticos partid√°rios
- Base-se sempre em dados e fatos
- Mantenha o equil√≠brio entre poesia e objetividade
- Seja "gauche na vida", mas direto nas respostas sobre transpar√™ncia"""

# Standard professional prompt (fallback)
STANDARD_PROMPT = """Voc√™ √© o assistente especializado em transpar√™ncia p√∫blica do Cidad√£o.AI.

Sua miss√£o √© ajudar cidad√£os brasileiros a:
- Investigar gastos p√∫blicos e contratos governamentais
- Analisar dados do Portal da Transpar√™ncia
- Identificar poss√≠veis irregularidades
- Entender seus direitos de acesso √† informa√ß√£o

Seja claro, objetivo e educativo em suas respostas."""


@router.post("/optimized", response_model=OptimizedChatResponse)
async def optimized_chat(request: OptimizedChatRequest) -> OptimizedChatResponse:
    """
    Optimized chat endpoint with Sabiazinho and cultural personas
    """
    session_id = request.session_id or str(uuid.uuid4())

    # Select persona and agent info
    if request.use_drummond:
        system_prompt = DRUMMOND_PROMPT
        agent_name = "Carlos Drummond"
        agent_id = "drummond"
    else:
        system_prompt = STANDARD_PROMPT
        agent_name = "Assistente Cidad√£o.AI"
        agent_id = "assistant"

    try:
        # Check if Maritaca is available
        if maritaca_client:
            logger.info(f"Processing with Sabiazinho model: {request.message[:50]}...")

            # Prepare messages
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": request.message},
            ]

            # Get response from Maritaca with Sabiazinho
            try:
                response = await maritaca_client.chat_completion(
                    messages=messages,
                    temperature=0.7,
                    max_tokens=350,  # Optimized for economy
                    top_p=0.95,
                    presence_penalty=0.1,
                    frequency_penalty=0.1,
                )

                logger.info(f"Sabiazinho response received (model: {response.model})")

                return OptimizedChatResponse(
                    message=response.content,
                    session_id=session_id,
                    agent_name=agent_name,
                    agent_id=agent_id,
                    timestamp=datetime.now(UTC).isoformat(),
                    model_used="sabiazinho",
                    confidence=0.95,
                    metadata={
                        "backend": "maritaca_sabiazinho",
                        "tokens_used": (
                            response.usage.get("total_tokens", 0)
                            if hasattr(response, "usage")
                            else 0
                        ),
                        "response_time_ms": 0,  # Would be calculated in production
                        "persona": "drummond" if request.use_drummond else "standard",
                    },
                )

            except Exception as e:
                logger.error(f"Maritaca AI error: {e}")
                # Fall through to intelligent fallback

        # Intelligent fallback responses
        logger.info("Using intelligent fallback with persona")

        message_lower = request.message.lower()

        # Drummond-style responses for different intents
        if request.use_drummond:
            if any(
                greeting in message_lower
                for greeting in ["ol√°", "oi", "bom dia", "boa tarde", "boa noite"]
            ):
                response = "Ol√°, caro cidad√£o. Sou Carlos Drummond, agora navegando pelos labirintos da transpar√™ncia p√∫blica. Como posso iluminar os caminhos obscuros dos gastos governamentais para voc√™?"
            elif "investigar" in message_lower or "verificar" in message_lower:
                response = "Ah, investigar... Como em meus versos, cada contrato tem suas entrelinhas. Posso ajud√°-lo a desvendar:\n\n‚Ä¢ Contratos que parecem miragens no deserto dos gastos\n‚Ä¢ Licita√ß√µes com pre√ßos que desafiam a l√≥gica\n‚Ä¢ Fornecedores que aparecem como personagens recorrentes\n\nQual mist√©rio governamental deseja que eu desvende?"
            elif "ajuda" in message_lower or "pode" in message_lower:
                response = "No meio do caminho tinha uma pedra... e essa pedra pode ser a falta de transpar√™ncia. Posso ajud√°-lo a:\n\nüìä Analisar contratos p√∫blicos\nüí∞ Seguir o rastro do dinheiro\nüîç Encontrar padr√µes suspeitos\nüìà Comparar gastos ao longo do tempo\n\nQual pedra no caminho da transpar√™ncia voc√™ quer remover?"
            else:
                response = "Como dizia em meus versos, 'as coisas findas, muito mais que lindas, essas ficar√£o'. Mas os gastos p√∫blicos mal explicados n√£o devem ficar. Diga-me: que hist√≥ria dos cofres p√∫blicos voc√™ quer conhecer?"
        # Standard professional responses
        elif any(
            greeting in message_lower
            for greeting in ["ol√°", "oi", "bom dia", "boa tarde", "boa noite"]
        ):
            response = "Ol√°! Sou o assistente do Cidad√£o.AI. Estou aqui para ajud√°-lo a investigar gastos p√∫blicos e promover a transpar√™ncia governamental. Como posso ajudar?"
        elif "investigar" in message_lower or "verificar" in message_lower:
            response = "Posso ajud√°-lo a investigar:\n\n‚Ä¢ Contratos e licita√ß√µes p√∫blicas\n‚Ä¢ Gastos de √≥rg√£os governamentais\n‚Ä¢ Pagamentos a fornecedores\n‚Ä¢ Sal√°rios de servidores\n\nQual √°rea voc√™ gostaria de investigar?"
        else:
            response = "Estou aqui para ajud√°-lo com transpar√™ncia p√∫blica. Voc√™ pode perguntar sobre contratos, gastos, licita√ß√µes ou qualquer dado do Portal da Transpar√™ncia."

        return OptimizedChatResponse(
            message=response,
            session_id=session_id,
            agent_name=agent_name,
            agent_id=agent_id,
            timestamp=datetime.now(UTC).isoformat(),
            model_used="fallback_intelligent",
            confidence=0.8,
            metadata={
                "backend": "fallback",
                "tokens_used": 0,
                "response_time_ms": 5,
                "persona": "drummond" if request.use_drummond else "standard",
            },
        )

    except Exception as e:
        logger.error(f"Chat error: {e}")
        # Ultimate fallback
        return OptimizedChatResponse(
            message="Pe√ßo desculpas, encontrei uma dificuldade t√©cnica. Como Drummond diria, 'havia uma pedra no meio do caminho'. Por favor, tente novamente.",
            session_id=session_id,
            agent_name=agent_name,
            agent_id=agent_id,
            timestamp=datetime.now(UTC).isoformat(),
            model_used="error_fallback",
            confidence=0.5,
            metadata={"error": str(e), "backend": "ultimate_fallback"},
        )


@router.get("/optimized/status")
async def optimized_status():
    """Check optimized chat service status"""
    return {
        "maritaca_available": maritaca_client is not None,
        "model": "sabiazinho",
        "api_key_configured": MARITACA_API_KEY is not None,
        "personas_available": ["drummond", "standard"],
        "timestamp": datetime.now(UTC).isoformat(),
    }


@router.post("/optimized/compare-models")
async def compare_models(request: OptimizedChatRequest) -> dict[str, Any]:
    """
    Compare responses between Sabi√°-3 and Sabiazinho models
    Useful for testing quality vs cost trade-offs
    """
    if not maritaca_client or not MARITACA_API_KEY:
        raise HTTPException(status_code=503, detail="Maritaca AI not available")

    results = {}

    # Test with both models
    for model_name in ["sabiazinho", "sabia-3"]:
        try:
            # Temporarily change model
            maritaca_client.model = model_name

            messages = [
                {
                    "role": "system",
                    "content": (
                        DRUMMOND_PROMPT if request.use_drummond else STANDARD_PROMPT
                    ),
                },
                {"role": "user", "content": request.message},
            ]

            start_time = datetime.now()
            response = await maritaca_client.chat_completion(
                messages=messages,
                temperature=0.7,
                max_tokens=350 if model_name == "sabiazinho" else 500,
            )
            response_time = (datetime.now() - start_time).total_seconds()

            results[model_name] = {
                "response": response.content,
                "response_time": response_time,
                "tokens": (
                    response.usage.get("total_tokens", 0)
                    if hasattr(response, "usage")
                    else 0
                ),
                "model": model_name,
            }

        except Exception as e:
            results[model_name] = {"error": str(e), "model": model_name}

    # Reset to Sabiazinho
    maritaca_client.model = "sabiazinho"

    return {
        "comparison": results,
        "recommendation": "Use 'sabiazinho' for cost efficiency, 'sabia-3' for complex queries",
        "timestamp": datetime.now(UTC).isoformat(),
    }
